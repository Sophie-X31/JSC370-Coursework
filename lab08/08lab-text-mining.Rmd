---
title: "Lab 08 - Text Mining/NLP"
# output: html_document
output: github_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(eval = F, include  = T)
```

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text
- Use dplyr and ggplot2 to analyze and visualize text data
- Try a theme model using `topicmodels`

# Lab description

For this lab we will be working with the medical record transcriptions from https://www.mtsamples.com/ available at https://github.com/JSC370/JSC370-2025/tree/main/data/medical_transcriptions.

# Deliverables

1. Questions 1-7 answered, knit to pdf or html output uploaded to Quercus.

2. Render the Rmarkdown document using `github_document` and add it to your github site. Add link to github site in your html.


### Setup packages

You should load in `tidyverse`, (or `data.table`), `tidytext`, `wordcloud2`, `tm`, and `topicmodels`.


## Read in the Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r eval=TRUE, message=FALSE, echo=FALSE}
library(tidytext)
library(tidyverse)
library(wordcloud2)
library(tm)
library(topicmodels)
library(tokenizers)
library(reshape2)
library(stringr)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2025/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples |>
  select(description, medical_specialty, transcription)

head(mt_samples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different medical specialties are in the data. Are these categories related? overlapping? evenly distributed? Make a bar plot.

```{r eval=TRUE, message=FALSE, echo=FALSE}
mt_samples |>
  count(medical_specialty, sort = TRUE) |>
  ggplot(aes(fct_reorder(medical_specialty, n), n)) +
  geom_col(fill="lightgreen") +
  coord_flip() +
  theme_bw()
```

**Answer:** __The data isn't evenly distributed, most of the transcriptions are from the surgeries. Also, there aren't much relation between categories, e.g. surgery and neurosurgery is only somewhat related. The categories seem like distinct specialties, and there isn't a lot of overlapping.__

---

## Question 2: Tokenize

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words with a bar plot
- Create a word cloud of the top 20 most frequent words

### Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r eval=TRUE, message=FALSE, echo=FALSE}
tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  group_by(word) |>
  summarise(word_frequency = n()) |>
  arrange(across(word_frequency, desc)) 

tokens_20 <- tokens |> head(20)
tokens_20 |>
  ggplot(aes(fct_reorder(word, word_frequency), word_frequency)) +
  geom_bar(stat="identity", fill="lightgreen") +
  coord_flip() +
  theme_bw()

tokens_20 |>
  count(word, sort=TRUE) |>
  wordcloud2(size=0.5, color="random-light", background="grey")
```

**Answer:** __We see that these most frequent words are mostly stop words that do not have any meaning in the medical context.__

---

## Question 3: Stopwords

- Redo Question 2 but remove stopwords
- Check `stopwords()` library and `stop_words` in `tidytext`
- Use regex to remove numbers as well
- Try customizing your stopwords list to include 3-4 additional words that do not appear informative

### What do we see when you remove stopwords and then when you filter further? Does it give us a better idea of what the text is about?

```{r eval=TRUE, message=FALSE, echo=FALSE}
#head(stopwords("english"))
#length(stopwords("english"))
#head(stop_words)

tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  filter(!str_detect(word, "['’]")) |> 
  filter(!str_detect(word, "[[:digit:]]+")) |>
  filter(!word %in% c("mm", "mg", "noted")) |>
  anti_join(stop_words, by = "word") |>
  group_by(word) |>
  summarise(word_frequency = n()) |>
  arrange(across(word_frequency, desc))

tokens_20 <- tokens |> head(20)
tokens_20 |>
  ggplot(aes(fct_reorder(word, word_frequency), word_frequency)) +
  geom_bar(stat="identity", fill="lightgreen") +
  coord_flip() +
  theme_bw()

tokens_20 |>
  count(word, sort=TRUE) |>
  wordcloud2(size=0.5, color="random-light", background="grey")
```


**Answer:** __The filtered result is now composed of mostly medical-related words, which better represents the text. We can also see the word count has drastically decreased (from 120,000 to 15,000).__

---

## Question 4: ngrams

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams? Note we need to remove stopwords a little differently. You don't need to recreate the wordclouds.

```{r eval=TRUE, message=FALSE, echo=FALSE}
bigrams <- mt_samples$transcription |>
  tokenize_ngrams(n = 2) |>
  unlist() |>
  table() |>
  as.data.frame()
colnames(bigrams) <- c("bigram", "frequency")

bigrams <- bigrams |> 
  separate(bigram, into = c("word1", "word2"), sep = " ", remove = FALSE) |>
  filter(!str_detect(bigram, "['’]")) |>
  filter(!str_detect(bigram, "[[:digit:]]+")) |>
  filter(!word1 %in% c("mm", "mg", "noted")) |>
  filter(!word2 %in% c("mm", "mg", "noted")) |>  
  anti_join(stop_words, by = c("word1" = "word")) |>
  anti_join(stop_words, by = c("word2" = "word")) |>
  arrange(desc(frequency))

bigrams_20 <- bigrams |> head(20)
bigrams_20 |>
  ggplot(aes(fct_reorder(bigram, frequency), frequency)) +
  geom_col(fill="lightgreen") +
  coord_flip() +
  theme_bw()
```

```{r eval=FALSE, message=FALSE, echo=FALSE}
## TUTORIAL CODE SLOW ##
#stopwords2 <- c("mm", "mg", "noted", stop_words$word)
#sw_start <- paste0("^", paste(stopwords2, collapse=" |^"), "$")
#sw_end <- paste0("", paste(stopwords2, collapse="$| "), "$")

#tokens_bigram <- mt_samples |>
#  select(transcription) |>
#  unnest_ngrams(ngram, transcription, n = 2) |>
#  filter(!grepl(sw_start, ngram, ignore.case=TRUE)) |>
#  filter(!grepl(sw_end, ngram, ignore.case=TRUE)) |>
#  filter(!grepl("[[:digit:]]+", ngram)) |>
#  filter(!grepl("['’]", ngram)) |>
#  group_by(ngram) |>
#  summarise(word_frequency = n()) |>
#  arrange(across(word_frequency, desc)) |>
#  head(20)
```


**Answer:** __The maximum word count has been further reduced, and the bigrams have more insights as the combinations of words provide more context than a single word. Note we are NOT doing the trigrams anymore as instructed in the tutorial.__

---

## Question 5: Examining words

Using the results from the bigram, pick a word and count the words that appear before and after it, and create a plot of the top 20.

```{r eval=TRUE, message=FALSE, echo=FALSE}
pick_bigram <- bigrams |>
  filter(str_detect(bigram, "\\b blood$|^blood \\b")) |>
  mutate(before = ifelse(str_detect(word1, "blood"), word2, word1)) |>
  group_by(before) |> 
  summarise(word_frequency = sum(frequency)) |>
  arrange(desc(word_frequency)) |>  
  head(20)

pick_bigram |>
  ggplot(aes(x = fct_reorder(before, word_frequency), y = word_frequency)) +
  geom_col(fill = "lightgreen") +
  coord_flip() +
  theme_bw()
```

**Answer:** __We can see some common combinations, e.g. blood pressure, white/red blood cell, etc.__

---

## Question 6: Words by Specialties

Which words are most used in each of the specialties? You can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the 5 most used words?

```{r eval=TRUE, message=FALSE, echo=FALSE}
mt_samples |>
  unnest_tokens(word, transcription) |>
  filter(!str_detect(word, "['’]")) |> 
  filter(!str_detect(word, "[[:digit:]]+")) |>
  filter(!word %in% c("mm", "mg", "noted")) |>
  anti_join(stop_words, by = "word") |>
  group_by(medical_specialty, word) |>
  summarise(word_frequency = n()) |>
  group_by(medical_specialty) |>
  slice_max(word_frequency, n = 5, with_ties = FALSE) |> # or top_n(5, word_frequency)
  arrange(medical_specialty, desc(word_frequency))
```


**Answer:** __For example, for the Allergy / Immunology specialty, the most used five words are "allergies", "allegra", "prescription", "sprays", and "erythematous", which is reasonable.__

---

## Question 7: Topic Models

See if there are any themes in the data by using a topic model (LDA). 

- you first need to create a document term matrix
- then you can try the LDA function in `topicmodels`. Try different k values.
- create a facet plot of the results from the LDA (see code from lecture)

```{r eval=TRUE, message=FALSE, echo=FALSE}
transcripts_dtm <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  filter(!str_detect(word, "['’]")) |> 
  filter(!str_detect(word, "[[:digit:]]+")) |>
  filter(!word %in% c("mm", "mg", "noted")) |>
  anti_join(stop_words, by = "word") |>
  DocumentTermMatrix() |>
  as.matrix()
```

```{r eval=TRUE, message=FALSE, echo=FALSE}
transcripts_lda <- LDA(transcripts_dtm, k=3, control=list(seed=42))
top_terms <- 
  tidy(transcripts_lda, matrix="beta") |>
  group_by(topic) |>
  slice_max(beta, n=10) |>
  ungroup() |>
  arrange(topic, -beta)

top_terms |>
  mutate(term=reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill=factor(topic))) + 
  geom_col(snow.legend=FALSE) +
  facet_wrap(~topic, scales="free") +
  scale_y_reordered() +
  theme_bw()
```


**Answer:** __We can sort of see the themes of the generated topic groupings. Topic 1 resolves around surgical procedures, topic 2 is mostly about medical history and physical examination, while topic 3 has top words regarding postoperative care and patient management.__

---



